import os
import json
import cv2
import numpy as np
import torch
from insightface.app import FaceAnalysis
import time


# âœ… ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "model")
INSIGHTFACE_DIR = os.path.join(MODEL_DIR, "insightface")

# âœ… InsightFaceê°€ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šë„ë¡ ê°•ì œ ì„¤ì •
os.environ["INSIGHTFACE_HOME"] = INSIGHTFACE_DIR

# âœ… ArcFace ëª¨ë¸ ë¡œë“œ (ë¡œì»¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°, **name="buffalo_l"ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•¨**)
arcface_app = FaceAnalysis(name="buffalo_l", root=INSIGHTFACE_DIR)  # âœ… ëª¨ë¸ ì´ë¦„ì„ ëª…í™•í•˜ê²Œ ì§€ì •
arcface_app.prepare(ctx_id=-1)  # CPU ì‚¬ìš©

print("âœ… YOLO ë° ArcFace ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

EMBEDDING_DIR = os.path.join(BASE_DIR, "embeddings")
os.makedirs(EMBEDDING_DIR, exist_ok=True)

MAX_CAPTURE = 5  # ìµœëŒ€ 5ì¥ í‰ê· 

def adjust_bbox_for_retina(yolo_bbox, scale_factor=1.2):
    """ YOLOì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ 20% í™•ëŒ€í•˜ì—¬ RetinaFaceì™€ ìœ ì‚¬í•œ í¬ê¸°ë¡œ ë³€í™˜ """
    x, y, w, h = yolo_bbox
    cx, cy = x + w / 2, y + h / 2
    new_w, new_h = w * scale_factor, h * scale_factor
    return [max(0, cx - new_w / 2), max(0, cy - new_h / 2), new_w, new_h]

def extract_faces_and_embeddings(image):
    """ ì‚¬ìš©ìì˜ ì–¼êµ´ì„ YOLOë¡œ ê²€ì¶œ í›„ ì„ë² ë”© ì¶”ì¶œí•˜ê³  í‰ê·  ê³„ì‚° """
    embeddings_list = []

    faces = arcface_app.get(image)
    if not faces:
        print("âŒ ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨")
        return None
    
    print("ì–¼êµ´ ê°ì§€ ì„±ê³µ")
    
    sorted_faces = sorted(faces, key=lambda f: (f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]), reverse=True)
    
    
    for face in sorted_faces[:MAX_CAPTURE]:
        embeddings_list.append(face.normed_embedding)


    if len(embeddings_list) == 0:
        return None

    # í‰ê·  ì„ë² ë”© ê³„ì‚°
    avg_embedding = np.mean(embeddings_list, axis=0)
    avg_embedding = avg_embedding / np.linalg.norm(avg_embedding)
    return avg_embedding.tolist()

def mask_matching_face(image, family_embeddings, mask_type="black", threshold=0.5, emojis=None):
    faces = arcface_app.get(image)
    print(f"ğŸ” ì–¼êµ´ ê°ì§€ë¨: {len(faces)}ê°œ")

    if not faces:
        return image

    for face in faces:
        if "embedding" not in face:
            continue

        face_embedding = np.array(face["embedding"])
        face_embedding = face_embedding / np.linalg.norm(face_embedding)

        for user_id, emb in family_embeddings.items():
            sim = np.dot(face_embedding, emb)
            if sim >= threshold:
                print(f"âœ… ìœ ì‚¬í•œ ê°€ì¡± êµ¬ì„±ì› íƒì§€ë¨ (user_id={user_id}, sim={sim:.3f})")

                # ë§ˆìŠ¤í‚¹ ì ìš©
                x_min, y_min, x_max, y_max = map(int, face["bbox"])
                w, h = x_max - x_min, y_max - y_min

                if mask_type == "black":
                    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 0), -1)
                elif mask_type == "blur":
                    face_roi = image[y_min:y_max, x_min:x_max]
                    face_roi = cv2.GaussianBlur(face_roi, (55, 55), 30)
                    image[y_min:y_max, x_min:x_max] = face_roi
                elif mask_type in emojis and emojis[mask_type] is not None:
                    emoji = cv2.resize(emojis[mask_type], (w, h))
                    image[y_min:y_max, x_min:x_max] = emoji

                break  # âœ… í•´ë‹¹ ì–¼êµ´ì€ ë§ˆìŠ¤í‚¹í–ˆìœ¼ë‹ˆ ë‹¤ìŒ ì–¼êµ´ë¡œ

    return image