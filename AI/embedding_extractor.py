import os
import json
import cv2
import numpy as np
import torch
from insightface.app import FaceAnalysis
import time


# âœ… ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "model")
INSIGHTFACE_DIR = os.path.join(MODEL_DIR, "insightface")

# âœ… InsightFaceê°€ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šë„ë¡ ê°•ì œ ì„¤ì •
os.environ["INSIGHTFACE_HOME"] = INSIGHTFACE_DIR

# âœ… YOLOv5 ëª¨ë¸ ë¡œë“œ (ë¡œì»¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
YOLO_MODEL_PATH = os.path.join(MODEL_DIR, "yolo", "yolov5s.pt")
yolo_model = torch.hub.load("ultralytics/yolov5", "custom", path=YOLO_MODEL_PATH, force_reload=False)

# âœ… ArcFace ëª¨ë¸ ë¡œë“œ (ë¡œì»¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°, **name="buffalo_l"ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•¨**)
arcface_app = FaceAnalysis(name="buffalo_l", root=INSIGHTFACE_DIR)  # âœ… ëª¨ë¸ ì´ë¦„ì„ ëª…í™•í•˜ê²Œ ì§€ì •
arcface_app.prepare(ctx_id=-1)  # CPU ì‚¬ìš©

print("âœ… YOLO ë° ArcFace ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

EMBEDDING_DIR = os.path.join(BASE_DIR, "embeddings")
os.makedirs(EMBEDDING_DIR, exist_ok=True)

MAX_CAPTURE = 5  # ìµœëŒ€ 5ì¥ í‰ê· 

def adjust_bbox_for_retina(yolo_bbox, scale_factor=1.2):
    """ YOLOì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ 20% í™•ëŒ€í•˜ì—¬ RetinaFaceì™€ ìœ ì‚¬í•œ í¬ê¸°ë¡œ ë³€í™˜ """
    x, y, w, h = yolo_bbox
    cx, cy = x + w / 2, y + h / 2
    new_w, new_h = w * scale_factor, h * scale_factor
    return [max(0, cx - new_w / 2), max(0, cy - new_h / 2), new_w, new_h]

def extract_faces_and_embeddings(image):
    """ ì‚¬ìš©ìì˜ ì–¼êµ´ì„ YOLOë¡œ ê²€ì¶œ í›„ ì„ë² ë”© ì¶”ì¶œí•˜ê³  í‰ê·  ê³„ì‚° """
    embeddings_list = []

    # YOLO ì–¼êµ´ ê²€ì¶œ
    results = yolo_model(image)
    
    if results is None or len(results.pred[0]) == 0:
        return None  # ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜

    # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
    detected_faces = sorted(results.pred[0].cpu().numpy(), key=lambda x: (x[2] - x[0]) * (x[3] - x[1]), reverse=True)
    
    for face in detected_faces[:MAX_CAPTURE]:
        x, y, w, h, conf, cls = face
        x, y, w, h = adjust_bbox_for_retina([x, y, w - x, h - y])  # ë°”ìš´ë”© ë°•ìŠ¤ í™•ëŒ€

        # ì–¼êµ´ í¬ë¡­
        face_crop = image[int(y):int(y + h), int(x):int(x + w)]

        # ArcFace ì„ë² ë”© ì¶”ì¶œ
        face_data = arcface_app.get(face_crop)
        if face_data:
            embeddings_list.append(face_data[0].normed_embedding)

        if len(embeddings_list) >= MAX_CAPTURE:
            break

    if len(embeddings_list) == 0:
        return None

    # í‰ê·  ì„ë² ë”© ê³„ì‚°
    avg_embedding = np.mean(embeddings_list, axis=0).tolist()
    return avg_embedding

def mask_matching_face(image, family_embeddings, mask_type="black", threshold=0.5, emojis=None):
    faces = arcface_app.get(image)
    print(f"ğŸ” ì–¼êµ´ ê°ì§€ë¨: {len(faces)}ê°œ")

    if not faces:
        return image

    for face in faces:
        if "embedding" not in face:
            continue

        face_embedding = np.array(face["embedding"])
        face_embedding = face_embedding / np.linalg.norm(face_embedding)

        for user_id, emb in family_embeddings.items():
            sim = np.dot(face_embedding, emb)
            if sim >= threshold:
                print(f"âœ… ìœ ì‚¬í•œ ê°€ì¡± êµ¬ì„±ì› íƒì§€ë¨ (user_id={user_id}, sim={sim:.3f})")

                # ë§ˆìŠ¤í‚¹ ì ìš©
                x_min, y_min, x_max, y_max = map(int, face["bbox"])
                w, h = x_max - x_min, y_max - y_min

                if mask_type == "black":
                    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 0), -1)
                elif mask_type == "blur":
                    face_roi = image[y_min:y_max, x_min:x_max]
                    face_roi = cv2.GaussianBlur(face_roi, (55, 55), 30)
                    image[y_min:y_max, x_min:x_max] = face_roi
                elif mask_type in emojis and emojis[mask_type] is not None:
                    emoji = cv2.resize(emojis[mask_type], (w, h))
                    image[y_min:y_max, x_min:x_max] = emoji

                break  # âœ… í•´ë‹¹ ì–¼êµ´ì€ ë§ˆìŠ¤í‚¹í–ˆìœ¼ë‹ˆ ë‹¤ìŒ ì–¼êµ´ë¡œ

    return image